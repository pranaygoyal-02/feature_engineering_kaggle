{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d1f0cdb",
   "metadata": {
    "papermill": {
     "duration": 0.005552,
     "end_time": "2023-04-20T18:11:06.362153",
     "exception": false,
     "start_time": "2023-04-20T18:11:06.356601",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Introduction #\n",
    "\n",
    "In the previous lesson we looked at our first model-based method for feature engineering: clustering. In this lesson we look at our next: principal component analysis (PCA). Just like clustering is a partitioning of the dataset based on proximity, you could think of PCA as a partitioning of the variation in the data. PCA is a great tool to help you discover important relationships in the data and can also be used to create more informative features.\n",
    "\n",
    "(Technical note: PCA is typically applied to [standardized](https://www.kaggle.com/alexisbcook/scaling-and-normalization) data. With standardized data \"variation\" means \"correlation\". With unstandardized data \"variation\" means \"covariance\". All data in this course will be standardized before applying PCA.)\n",
    "\n",
    "# Principal Component Analysis #\n",
    "\n",
    "In the [*Abalone*](https://www.kaggle.com/rodolfomendes/abalone-dataset) dataset are physical measurements taken from several thousand Tasmanian abalone. (An abalone is a sea creature much like a clam or an oyster.) We'll just look at a couple features for now: the `'Height'` and `'Diameter'` of their shells.\n",
    "\n",
    "You could imagine that within this data are \"axes of variation\" that describe the ways the abalone tend to differ from one another. Pictorially, these axes appear as perpendicular lines running along the natural dimensions of the data, one axis for each original feature.\n",
    "\n",
    "<figure style=\"padding: 1em;\">\n",
    "<img src=\"https://storage.googleapis.com/kaggle-media/learn/images/rr8NCDy.png\" width=300, alt=\"\">\n",
    "<figcaption style=\"textalign: center; font-style: italic\"><center>\n",
    "</center></figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36280605",
   "metadata": {
    "papermill": {
     "duration": 0.00396,
     "end_time": "2023-04-20T18:11:06.370683",
     "exception": false,
     "start_time": "2023-04-20T18:11:06.366723",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Often, we can give names to these axes of variation. The longer axis we might call the \"Size\" component: small height and small diameter (lower left) contrasted with large height and large diameter (upper right). The shorter axis we might call the \"Shape\" component: small height and large diameter (flat shape) contrasted with large height and small diameter (round shape).\n",
    "\n",
    "Notice that instead of describing abalones by their `'Height'` and `'Diameter'`, we could just as well describe them by their `'Size'` and `'Shape'`. This, in fact, is the whole idea of PCA: instead of describing the data with the original features, we describe it with its axes of variation. The axes of variation become the new features.\n",
    "\n",
    "<figure style=\"padding: 1em;\">\n",
    "<img src=\"https://storage.googleapis.com/kaggle-media/learn/images/XQlRD1q.png\" width=600, alt=\"\">\n",
    "<figcaption style=\"textalign: center; font-style: italic\"><center>The principal components become the new features by a rotation of the dataset in the feature space.\n",
    "</center></figcaption>\n",
    "</figure>\n",
    "\n",
    "The new features PCA constructs are actually just linear combinations (weighted sums) of the original features:\n",
    "\n",
    "```\n",
    "df[\"Size\"] = 0.707 * X[\"Height\"] + 0.707 * X[\"Diameter\"]\n",
    "df[\"Shape\"] = 0.707 * X[\"Height\"] - 0.707 * X[\"Diameter\"]\n",
    "```\n",
    "\n",
    "These new features are called the **principal components** of the data. The weights themselves are called **loadings**. There will be as many principal components as there are features in the original dataset: if we had used ten features instead of two, we would have ended up with ten components.\n",
    "\n",
    "A component's loadings tell us what variation it expresses through signs and magnitudes:\n",
    "\n",
    "| Features \\ Components | Size (PC1) | Shape (PC2) |\n",
    "|-----------------------|------------|-------------|\n",
    "| Height                | 0.707      | 0.707       |\n",
    "| Diameter              | 0.707      | -0.707      |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fbc92f3",
   "metadata": {
    "papermill": {
     "duration": 0.004043,
     "end_time": "2023-04-20T18:11:06.378945",
     "exception": false,
     "start_time": "2023-04-20T18:11:06.374902",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "This table of loadings is telling us that in the `Size` component, `Height` and `Diameter` vary in the same direction (same sign), but in the `Shape` component they vary in opposite directions (opposite sign). In each component, the loadings are all of the same magnitude and so the features contribute equally in both.\n",
    "\n",
    "PCA also tells us the *amount* of variation in each component. We can see from the figures that there is more variation in the data along the `Size` component than along the `Shape` component. PCA makes this precise through each component's **percent of explained variance**.\n",
    "\n",
    "<figure style=\"padding: 1em;\">\n",
    "<img src=\"https://storage.googleapis.com/kaggle-media/learn/images/xWTvqDA.png\" width=600, alt=\"\">\n",
    "<figcaption style=\"textalign: center; font-style: italic\"><center> Size accounts for about 96% and the Shape for about 4% of the variance between Height and Diameter.\n",
    "</center></figcaption>\n",
    "</figure>\n",
    "\n",
    "The `Size` component captures the majority of the variation between `Height` and `Diameter`. It's important to remember, however, that the amount of variance in a component doesn't necessarily correspond to how good it is as a predictor: it depends on what you're trying to predict.\n",
    "\n",
    "# PCA for Feature Engineering #\n",
    "\n",
    "There are two ways you could use PCA for feature engineering.\n",
    "\n",
    "The first way is to use it as a descriptive technique. Since the components tell you about the variation, you could compute the MI scores for the components and see what kind of variation is most predictive of your target. That could give you ideas for kinds of features to create -- a product of `'Height'` and `'Diameter'` if `'Size'` is important, say, or a ratio of `'Height'` and `'Diameter'` if `Shape` is important. You could even try clustering on one or more of the high-scoring components.\n",
    "\n",
    "The second way is to use the components themselves as features. Because the components expose the variational structure of the data directly, they can often be more informative than the original features. Here are some use-cases:\n",
    "- **Dimensionality reduction**: When your features are highly redundant (*multicollinear*, specifically), PCA will partition out the redundancy into one or more near-zero variance components, which you can then drop since they will contain little or no information.\n",
    "- **Anomaly detection**: Unusual variation, not apparent from the original features, will often show up in the low-variance components. These components could be highly informative in an anomaly or outlier detection task.\n",
    "- **Noise reduction**: A collection of sensor readings will often share some common background noise. PCA can sometimes collect the (informative) signal into a smaller number of features while leaving the noise alone, thus boosting the signal-to-noise ratio.\n",
    "- **Decorrelation**: Some ML algorithms struggle with highly-correlated features. PCA transforms correlated features into uncorrelated components, which could be easier for your algorithm to work with.\n",
    "\n",
    "PCA basically gives you direct access to the correlational structure of your data. You'll no doubt come up with applications of your own!\n",
    "\n",
    "<blockquote style=\"margin-right:auto; margin-left:auto; background-color: #ebf9ff; padding: 1em; margin:24px;\">\n",
    "<strong>PCA Best Practices</strong><br>\n",
    "There are a few things to keep in mind when applying PCA:\n",
    "<ul>\n",
    "<li> PCA only works with numeric features, like continuous quantities or counts.\n",
    "<li> PCA is sensitive to scale. It's good practice to standardize your data before applying PCA, unless you know you have good reason not to.\n",
    "<li> Consider removing or constraining outliers, since they can have an undue influence on the results.\n",
    "</ul>\n",
    "</blockquote>\n",
    "\n",
    "# Example - 1985 Automobiles #\n",
    "\n",
    "In this example, we'll return to our [*Automobile*](https://www.kaggle.com/toramky/automobile-dataset) dataset and apply PCA, using it as a descriptive technique to discover features. We'll look at other use-cases in the exercise.\n",
    "\n",
    "This hidden cell loads the data and defines the functions `plot_variance` and `make_mi_scores`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "528e9fea",
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2023-04-20T18:11:06.390124Z",
     "iopub.status.busy": "2023-04-20T18:11:06.388891Z",
     "iopub.status.idle": "2023-04-20T18:11:07.945597Z",
     "shell.execute_reply": "2023-04-20T18:11:07.944366Z"
    },
    "papermill": {
     "duration": 1.565763,
     "end_time": "2023-04-20T18:11:07.948870",
     "exception": false,
     "start_time": "2023-04-20T18:11:06.383107",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pranay.goyal\\AppData\\Local\\Temp\\ipykernel_2540\\1052294196.py:9: MatplotlibDeprecationWarning: The seaborn styles shipped by Matplotlib are deprecated since 3.6, as they no longer correspond to the styles shipped by seaborn. However, they will remain available as 'seaborn-v0_8-<style>'. Alternatively, directly use the seaborn API instead.\n",
      "  plt.style.use(\"seaborn-whitegrid\")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from IPython.display import display\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "\n",
    "\n",
    "plt.style.use(\"seaborn-whitegrid\")\n",
    "plt.rc(\"figure\", autolayout=True)\n",
    "plt.rc(\n",
    "    \"axes\",\n",
    "    labelweight=\"bold\",\n",
    "    labelsize=\"large\",\n",
    "    titleweight=\"bold\",\n",
    "    titlesize=14,\n",
    "    titlepad=10,\n",
    ")\n",
    "\n",
    "\n",
    "def plot_variance(pca, width=8, dpi=100):\n",
    "    # Create figure\n",
    "    fig, axs = plt.subplots(1, 2)\n",
    "    n = pca.n_components_\n",
    "    grid = np.arange(1, n + 1)\n",
    "    # Explained variance\n",
    "    evr = pca.explained_variance_ratio_\n",
    "    axs[0].bar(grid, evr)\n",
    "    axs[0].set(\n",
    "        xlabel=\"Component\", title=\"% Explained Variance\", ylim=(0.0, 1.0)\n",
    "    )\n",
    "    # Cumulative Variance\n",
    "    cv = np.cumsum(evr)\n",
    "    axs[1].plot(np.r_[0, grid], np.r_[0, cv], \"o-\")\n",
    "    axs[1].set(\n",
    "        xlabel=\"Component\", title=\"% Cumulative Variance\", ylim=(0.0, 1.0)\n",
    "    )\n",
    "    # Set up figure\n",
    "    fig.set(figwidth=8, dpi=100)\n",
    "    return axs\n",
    "\n",
    "def make_mi_scores(X, y, discrete_features):\n",
    "    mi_scores = mutual_info_regression(X, y, discrete_features=discrete_features)\n",
    "    mi_scores = pd.Series(mi_scores, name=\"MI Scores\", index=X.columns)\n",
    "    mi_scores = mi_scores.sort_values(ascending=False)\n",
    "    return mi_scores\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"data/automobile_data.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9357906e",
   "metadata": {
    "papermill": {
     "duration": 0.004238,
     "end_time": "2023-04-20T18:11:07.957625",
     "exception": false,
     "start_time": "2023-04-20T18:11:07.953387",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "We've selected four features that cover a range of properties. Each of these features also has a high MI score with the target, `price`. We'll standardize the data since these features aren't naturally on the same scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cfc1309b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.copy()\n",
    "y = X.pop(\"price\")\n",
    "\n",
    "# Label encoding for categoricals\n",
    "for colname in X.select_dtypes(\"object\"):\n",
    "    X[colname], _ = X[colname].factorize()\n",
    "\n",
    "# All discrete features should now have integer dtypes (double-check this before using MI!)\n",
    "discrete_features = X.dtypes == int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b2168d25",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: '?'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m mi_scores \u001b[39m=\u001b[39m make_mi_scores(X, y, discrete_features)\n\u001b[0;32m      2\u001b[0m mi_scores[::\u001b[39m3\u001b[39m]  \u001b[39m# show a few features with their MI scores\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[15], line 43\u001b[0m, in \u001b[0;36mmake_mi_scores\u001b[1;34m(X, y, discrete_features)\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmake_mi_scores\u001b[39m(X, y, discrete_features):\n\u001b[1;32m---> 43\u001b[0m     mi_scores \u001b[39m=\u001b[39m mutual_info_regression(X, y, discrete_features\u001b[39m=\u001b[39;49mdiscrete_features)\n\u001b[0;32m     44\u001b[0m     mi_scores \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mSeries(mi_scores, name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mMI Scores\u001b[39m\u001b[39m\"\u001b[39m, index\u001b[39m=\u001b[39mX\u001b[39m.\u001b[39mcolumns)\n\u001b[0;32m     45\u001b[0m     mi_scores \u001b[39m=\u001b[39m mi_scores\u001b[39m.\u001b[39msort_values(ascending\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\pranay.goyal\\virtualenv\\venv1\\Lib\\site-packages\\sklearn\\feature_selection\\_mutual_info.py:388\u001b[0m, in \u001b[0;36mmutual_info_regression\u001b[1;34m(X, y, discrete_features, n_neighbors, copy, random_state)\u001b[0m\n\u001b[0;32m    312\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmutual_info_regression\u001b[39m(\n\u001b[0;32m    313\u001b[0m     X, y, \u001b[39m*\u001b[39m, discrete_features\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mauto\u001b[39m\u001b[39m\"\u001b[39m, n_neighbors\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m, copy\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, random_state\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m\n\u001b[0;32m    314\u001b[0m ):\n\u001b[0;32m    315\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Estimate mutual information for a continuous target variable.\u001b[39;00m\n\u001b[0;32m    316\u001b[0m \n\u001b[0;32m    317\u001b[0m \u001b[39m    Mutual information (MI) [1]_ between two random variables is a non-negative\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    386\u001b[0m \u001b[39m           of a Random Vector\", Probl. Peredachi Inf., 23:2 (1987), 9-16\u001b[39;00m\n\u001b[0;32m    387\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 388\u001b[0m     \u001b[39mreturn\u001b[39;00m _estimate_mi(X, y, discrete_features, \u001b[39mFalse\u001b[39;49;00m, n_neighbors, copy, random_state)\n",
      "File \u001b[1;32mc:\\Users\\pranay.goyal\\virtualenv\\venv1\\Lib\\site-packages\\sklearn\\feature_selection\\_mutual_info.py:255\u001b[0m, in \u001b[0;36m_estimate_mi\u001b[1;34m(X, y, discrete_features, discrete_target, n_neighbors, copy, random_state)\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_estimate_mi\u001b[39m(\n\u001b[0;32m    199\u001b[0m     X,\n\u001b[0;32m    200\u001b[0m     y,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    205\u001b[0m     random_state\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    206\u001b[0m ):\n\u001b[0;32m    207\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Estimate mutual information between the features and the target.\u001b[39;00m\n\u001b[0;32m    208\u001b[0m \n\u001b[0;32m    209\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    253\u001b[0m \u001b[39m           Data Sets\". PLoS ONE 9(2), 2014.\u001b[39;00m\n\u001b[0;32m    254\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 255\u001b[0m     X, y \u001b[39m=\u001b[39m check_X_y(X, y, accept_sparse\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mcsc\u001b[39;49m\u001b[39m\"\u001b[39;49m, y_numeric\u001b[39m=\u001b[39;49m\u001b[39mnot\u001b[39;49;00m discrete_target)\n\u001b[0;32m    256\u001b[0m     n_samples, n_features \u001b[39m=\u001b[39m X\u001b[39m.\u001b[39mshape\n\u001b[0;32m    258\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(discrete_features, (\u001b[39mstr\u001b[39m, \u001b[39mbool\u001b[39m)):\n",
      "File \u001b[1;32mc:\\Users\\pranay.goyal\\virtualenv\\venv1\\Lib\\site-packages\\sklearn\\utils\\validation.py:1122\u001b[0m, in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[0;32m   1102\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   1103\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mestimator_name\u001b[39m}\u001b[39;00m\u001b[39m requires y to be passed, but the target y is None\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1104\u001b[0m     )\n\u001b[0;32m   1106\u001b[0m X \u001b[39m=\u001b[39m check_array(\n\u001b[0;32m   1107\u001b[0m     X,\n\u001b[0;32m   1108\u001b[0m     accept_sparse\u001b[39m=\u001b[39maccept_sparse,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1119\u001b[0m     input_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mX\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   1120\u001b[0m )\n\u001b[1;32m-> 1122\u001b[0m y \u001b[39m=\u001b[39m _check_y(y, multi_output\u001b[39m=\u001b[39;49mmulti_output, y_numeric\u001b[39m=\u001b[39;49my_numeric, estimator\u001b[39m=\u001b[39;49mestimator)\n\u001b[0;32m   1124\u001b[0m check_consistent_length(X, y)\n\u001b[0;32m   1126\u001b[0m \u001b[39mreturn\u001b[39;00m X, y\n",
      "File \u001b[1;32mc:\\Users\\pranay.goyal\\virtualenv\\venv1\\Lib\\site-packages\\sklearn\\utils\\validation.py:1147\u001b[0m, in \u001b[0;36m_check_y\u001b[1;34m(y, multi_output, y_numeric, estimator)\u001b[0m\n\u001b[0;32m   1145\u001b[0m     _ensure_no_complex_data(y)\n\u001b[0;32m   1146\u001b[0m \u001b[39mif\u001b[39;00m y_numeric \u001b[39mand\u001b[39;00m y\u001b[39m.\u001b[39mdtype\u001b[39m.\u001b[39mkind \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mO\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m-> 1147\u001b[0m     y \u001b[39m=\u001b[39m y\u001b[39m.\u001b[39;49mastype(np\u001b[39m.\u001b[39;49mfloat64)\n\u001b[0;32m   1149\u001b[0m \u001b[39mreturn\u001b[39;00m y\n",
      "\u001b[1;31mValueError\u001b[0m: could not convert string to float: '?'"
     ]
    }
   ],
   "source": [
    "mi_scores = make_mi_scores(X, y, discrete_features)\n",
    "mi_scores[::3]  # show a few features with their MI scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "349359a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(205, 26)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1e8fb2c0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-20T18:11:07.968302Z",
     "iopub.status.busy": "2023-04-20T18:11:07.967898Z",
     "iopub.status.idle": "2023-04-20T18:11:07.987371Z",
     "shell.execute_reply": "2023-04-20T18:11:07.986118Z"
    },
    "papermill": {
     "duration": 0.028271,
     "end_time": "2023-04-20T18:11:07.990250",
     "exception": false,
     "start_time": "2023-04-20T18:11:07.961979",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['highway_mpg', 'engine_size', 'curb_weight'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m X \u001b[39m=\u001b[39m df\u001b[39m.\u001b[39mcopy()\n\u001b[0;32m      4\u001b[0m y \u001b[39m=\u001b[39m X\u001b[39m.\u001b[39mpop(\u001b[39m'\u001b[39m\u001b[39mprice\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m----> 5\u001b[0m X \u001b[39m=\u001b[39m X\u001b[39m.\u001b[39;49mloc[:, features]\n\u001b[0;32m      7\u001b[0m \u001b[39m# Standardize\u001b[39;00m\n\u001b[0;32m      8\u001b[0m X_scaled \u001b[39m=\u001b[39m (X \u001b[39m-\u001b[39m X\u001b[39m.\u001b[39mmean(axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)) \u001b[39m/\u001b[39m X\u001b[39m.\u001b[39mstd(axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\pranay.goyal\\virtualenv\\venv1\\Lib\\site-packages\\pandas\\core\\indexing.py:1067\u001b[0m, in \u001b[0;36m_LocationIndexer.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1065\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_is_scalar_access(key):\n\u001b[0;32m   1066\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobj\u001b[39m.\u001b[39m_get_value(\u001b[39m*\u001b[39mkey, takeable\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_takeable)\n\u001b[1;32m-> 1067\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_getitem_tuple(key)\n\u001b[0;32m   1068\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1069\u001b[0m     \u001b[39m# we by definition only have the 0th axis\u001b[39;00m\n\u001b[0;32m   1070\u001b[0m     axis \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maxis \u001b[39mor\u001b[39;00m \u001b[39m0\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\pranay.goyal\\virtualenv\\venv1\\Lib\\site-packages\\pandas\\core\\indexing.py:1256\u001b[0m, in \u001b[0;36m_LocIndexer._getitem_tuple\u001b[1;34m(self, tup)\u001b[0m\n\u001b[0;32m   1253\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_multi_take_opportunity(tup):\n\u001b[0;32m   1254\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_multi_take(tup)\n\u001b[1;32m-> 1256\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_getitem_tuple_same_dim(tup)\n",
      "File \u001b[1;32mc:\\Users\\pranay.goyal\\virtualenv\\venv1\\Lib\\site-packages\\pandas\\core\\indexing.py:924\u001b[0m, in \u001b[0;36m_LocationIndexer._getitem_tuple_same_dim\u001b[1;34m(self, tup)\u001b[0m\n\u001b[0;32m    921\u001b[0m \u001b[39mif\u001b[39;00m com\u001b[39m.\u001b[39mis_null_slice(key):\n\u001b[0;32m    922\u001b[0m     \u001b[39mcontinue\u001b[39;00m\n\u001b[1;32m--> 924\u001b[0m retval \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39;49m(retval, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\u001b[39m.\u001b[39;49m_getitem_axis(key, axis\u001b[39m=\u001b[39;49mi)\n\u001b[0;32m    925\u001b[0m \u001b[39m# We should never have retval.ndim < self.ndim, as that should\u001b[39;00m\n\u001b[0;32m    926\u001b[0m \u001b[39m#  be handled by the _getitem_lowerdim call above.\u001b[39;00m\n\u001b[0;32m    927\u001b[0m \u001b[39massert\u001b[39;00m retval\u001b[39m.\u001b[39mndim \u001b[39m==\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mndim\n",
      "File \u001b[1;32mc:\\Users\\pranay.goyal\\virtualenv\\venv1\\Lib\\site-packages\\pandas\\core\\indexing.py:1301\u001b[0m, in \u001b[0;36m_LocIndexer._getitem_axis\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1298\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(key, \u001b[39m\"\u001b[39m\u001b[39mndim\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mand\u001b[39;00m key\u001b[39m.\u001b[39mndim \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m   1299\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mCannot index with multidimensional key\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m-> 1301\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_getitem_iterable(key, axis\u001b[39m=\u001b[39;49maxis)\n\u001b[0;32m   1303\u001b[0m \u001b[39m# nested tuple slicing\u001b[39;00m\n\u001b[0;32m   1304\u001b[0m \u001b[39mif\u001b[39;00m is_nested_tuple(key, labels):\n",
      "File \u001b[1;32mc:\\Users\\pranay.goyal\\virtualenv\\venv1\\Lib\\site-packages\\pandas\\core\\indexing.py:1239\u001b[0m, in \u001b[0;36m_LocIndexer._getitem_iterable\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1236\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_key(key, axis)\n\u001b[0;32m   1238\u001b[0m \u001b[39m# A collection of keys\u001b[39;00m\n\u001b[1;32m-> 1239\u001b[0m keyarr, indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_listlike_indexer(key, axis)\n\u001b[0;32m   1240\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobj\u001b[39m.\u001b[39m_reindex_with_indexers(\n\u001b[0;32m   1241\u001b[0m     {axis: [keyarr, indexer]}, copy\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, allow_dups\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m\n\u001b[0;32m   1242\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\pranay.goyal\\virtualenv\\venv1\\Lib\\site-packages\\pandas\\core\\indexing.py:1432\u001b[0m, in \u001b[0;36m_LocIndexer._get_listlike_indexer\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1429\u001b[0m ax \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobj\u001b[39m.\u001b[39m_get_axis(axis)\n\u001b[0;32m   1430\u001b[0m axis_name \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobj\u001b[39m.\u001b[39m_get_axis_name(axis)\n\u001b[1;32m-> 1432\u001b[0m keyarr, indexer \u001b[39m=\u001b[39m ax\u001b[39m.\u001b[39;49m_get_indexer_strict(key, axis_name)\n\u001b[0;32m   1434\u001b[0m \u001b[39mreturn\u001b[39;00m keyarr, indexer\n",
      "File \u001b[1;32mc:\\Users\\pranay.goyal\\virtualenv\\venv1\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6070\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[1;34m(self, key, axis_name)\u001b[0m\n\u001b[0;32m   6067\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   6068\u001b[0m     keyarr, indexer, new_indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[1;32m-> 6070\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_raise_if_missing(keyarr, indexer, axis_name)\n\u001b[0;32m   6072\u001b[0m keyarr \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtake(indexer)\n\u001b[0;32m   6073\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(key, Index):\n\u001b[0;32m   6074\u001b[0m     \u001b[39m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\pranay.goyal\\virtualenv\\venv1\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6133\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[1;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[0;32m   6130\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mNone of [\u001b[39m\u001b[39m{\u001b[39;00mkey\u001b[39m}\u001b[39;00m\u001b[39m] are in the [\u001b[39m\u001b[39m{\u001b[39;00maxis_name\u001b[39m}\u001b[39;00m\u001b[39m]\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m   6132\u001b[0m not_found \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[39m.\u001b[39mnonzero()[\u001b[39m0\u001b[39m]]\u001b[39m.\u001b[39munique())\n\u001b[1;32m-> 6133\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mnot_found\u001b[39m}\u001b[39;00m\u001b[39m not in index\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mKeyError\u001b[0m: \"['highway_mpg', 'engine_size', 'curb_weight'] not in index\""
     ]
    }
   ],
   "source": [
    "features = [\"highway_mpg\", \"engine_size\", \"horsepower\", \"curb_weight\"]\n",
    "\n",
    "X = df.copy()\n",
    "y = X.pop('price')\n",
    "X = X.loc[:, features]\n",
    "\n",
    "# Standardize\n",
    "X_scaled = (X - X.mean(axis=0)) / X.std(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e9ea5b",
   "metadata": {
    "papermill": {
     "duration": 0.004328,
     "end_time": "2023-04-20T18:11:07.999140",
     "exception": false,
     "start_time": "2023-04-20T18:11:07.994812",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Now we can fit scikit-learn's `PCA` estimator and create the principal components. You can see here the first few rows of the transformed dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c76805b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-20T18:11:08.010579Z",
     "iopub.status.busy": "2023-04-20T18:11:08.009885Z",
     "iopub.status.idle": "2023-04-20T18:11:08.056391Z",
     "shell.execute_reply": "2023-04-20T18:11:08.054873Z"
    },
    "papermill": {
     "duration": 0.055448,
     "end_time": "2023-04-20T18:11:08.059146",
     "exception": false,
     "start_time": "2023-04-20T18:11:08.003698",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Create principal components\n",
    "pca = PCA()\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "# Convert to dataframe\n",
    "component_names = [f\"PC{i+1}\" for i in range(X_pca.shape[1])]\n",
    "X_pca = pd.DataFrame(X_pca, columns=component_names)\n",
    "\n",
    "X_pca.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25a8dc02",
   "metadata": {
    "papermill": {
     "duration": 0.004496,
     "end_time": "2023-04-20T18:11:08.068511",
     "exception": false,
     "start_time": "2023-04-20T18:11:08.064015",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "After fitting, the `PCA` instance contains the loadings in its `components_` attribute. (Terminology for PCA is inconsistent, unfortunately. We're following the convention that calls the transformed columns in `X_pca` the *components*, which otherwise don't have a name.) We'll wrap the loadings up in a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d486136",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-20T18:11:08.080166Z",
     "iopub.status.busy": "2023-04-20T18:11:08.079196Z",
     "iopub.status.idle": "2023-04-20T18:11:08.092818Z",
     "shell.execute_reply": "2023-04-20T18:11:08.091614Z"
    },
    "papermill": {
     "duration": 0.021879,
     "end_time": "2023-04-20T18:11:08.095030",
     "exception": false,
     "start_time": "2023-04-20T18:11:08.073151",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "loadings = pd.DataFrame(\n",
    "    pca.components_.T,  # transpose the matrix of loadings\n",
    "    columns=component_names,  # so the columns are the principal components\n",
    "    index=X.columns,  # and the rows are the original features\n",
    ")\n",
    "loadings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "597ee685",
   "metadata": {
    "papermill": {
     "duration": 0.004647,
     "end_time": "2023-04-20T18:11:08.104623",
     "exception": false,
     "start_time": "2023-04-20T18:11:08.099976",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Recall that the signs and magnitudes of a component's loadings tell us what kind of variation it's captured. The first component (`PC1`) shows a contrast between large, powerful vehicles with poor gas milage, and smaller, more economical vehicles with good gas milage. We might call this the \"Luxury/Economy\" axis. The next figure shows that our four chosen features mostly vary along the Luxury/Economy axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc20422",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-20T18:11:08.116442Z",
     "iopub.status.busy": "2023-04-20T18:11:08.115576Z",
     "iopub.status.idle": "2023-04-20T18:11:08.580668Z",
     "shell.execute_reply": "2023-04-20T18:11:08.579379Z"
    },
    "papermill": {
     "duration": 0.474127,
     "end_time": "2023-04-20T18:11:08.583519",
     "exception": false,
     "start_time": "2023-04-20T18:11:08.109392",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Look at explained variance\n",
    "plot_variance(pca);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8ca095a",
   "metadata": {
    "papermill": {
     "duration": 0.005327,
     "end_time": "2023-04-20T18:11:08.594444",
     "exception": false,
     "start_time": "2023-04-20T18:11:08.589117",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Let's also look at the MI scores of the components. Not surprisingly, `PC1` is highly informative, though the remaining components, despite their small variance, still have a significant relationship with `price`. Examining those components could be worthwhile to find relationships not captured by the main Luxury/Economy axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f25247",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-20T18:11:08.607981Z",
     "iopub.status.busy": "2023-04-20T18:11:08.607113Z",
     "iopub.status.idle": "2023-04-20T18:11:08.629098Z",
     "shell.execute_reply": "2023-04-20T18:11:08.627859Z"
    },
    "papermill": {
     "duration": 0.031662,
     "end_time": "2023-04-20T18:11:08.631618",
     "exception": false,
     "start_time": "2023-04-20T18:11:08.599956",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "mi_scores = make_mi_scores(X_pca, y, discrete_features=False)\n",
    "mi_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "087b0b4a",
   "metadata": {
    "papermill": {
     "duration": 0.005859,
     "end_time": "2023-04-20T18:11:08.646337",
     "exception": false,
     "start_time": "2023-04-20T18:11:08.640478",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "The third component shows a contrast between `horsepower` and `curb_weight` -- sports cars vs. wagons, it seems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8472daef",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-20T18:11:08.661554Z",
     "iopub.status.busy": "2023-04-20T18:11:08.660858Z",
     "iopub.status.idle": "2023-04-20T18:11:08.677888Z",
     "shell.execute_reply": "2023-04-20T18:11:08.676585Z"
    },
    "papermill": {
     "duration": 0.026844,
     "end_time": "2023-04-20T18:11:08.680283",
     "exception": false,
     "start_time": "2023-04-20T18:11:08.653439",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Show dataframe sorted by PC3\n",
    "idx = X_pca[\"PC3\"].sort_values(ascending=False).index\n",
    "cols = [\"make\", \"body_style\", \"horsepower\", \"curb_weight\"]\n",
    "df.loc[idx, cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5370c5e",
   "metadata": {
    "papermill": {
     "duration": 0.006081,
     "end_time": "2023-04-20T18:11:08.692649",
     "exception": false,
     "start_time": "2023-04-20T18:11:08.686568",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "To express this contrast, let's create a new ratio feature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7cd52c6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-20T18:11:08.707436Z",
     "iopub.status.busy": "2023-04-20T18:11:08.706705Z",
     "iopub.status.idle": "2023-04-20T18:11:09.236306Z",
     "shell.execute_reply": "2023-04-20T18:11:09.234975Z"
    },
    "papermill": {
     "duration": 0.540323,
     "end_time": "2023-04-20T18:11:09.239266",
     "exception": false,
     "start_time": "2023-04-20T18:11:08.698943",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df[\"sports_or_wagon\"] = X.curb_weight / X.horsepower\n",
    "sns.regplot(x=\"sports_or_wagon\", y='price', data=df, order=2);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab0ef6e",
   "metadata": {
    "papermill": {
     "duration": 0.007006,
     "end_time": "2023-04-20T18:11:09.254089",
     "exception": false,
     "start_time": "2023-04-20T18:11:09.247083",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Your Turn #\n",
    "\n",
    "[**Improve your feature set**](https://www.kaggle.com/kernels/fork/14393921) by decomposing the variation in *Ames Housing* and use principal components to detect outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a9641eb",
   "metadata": {
    "papermill": {
     "duration": 0.006923,
     "end_time": "2023-04-20T18:11:09.268549",
     "exception": false,
     "start_time": "2023-04-20T18:11:09.261626",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "---\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "*Have questions or comments? Visit the [course discussion forum](https://www.kaggle.com/learn/feature-engineering/discussion) to chat with other learners.*"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "formats": "ipynb"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 14.609608,
   "end_time": "2023-04-20T18:11:10.098566",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-04-20T18:10:55.488958",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
